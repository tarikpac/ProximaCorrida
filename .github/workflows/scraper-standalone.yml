# GitHub Actions Workflow: ProximaCorrida Multi-Provider Scraper
# Runs daily at 06:00 UTC (03:00 BRT) to scrape events from multiple providers
# This is the standalone version that doesn't require the API to be running.
# Push notifications are sent inline by calling the Lambda API.
#
# Required Secrets:
# - DATABASE_URL: Supabase PostgreSQL connection string (with pooler)
# - DIRECT_URL: Direct PostgreSQL connection string (without pooler)
# - API_BASE_URL: Lambda API Gateway URL (e.g., https://xxx.execute-api.sa-east-1.amazonaws.com)

name: Multi-Provider Scraper

on:
  # Daily execution at 06:00 UTC (03:00 BRT)
  schedule:
    - cron: '0 6 * * *'
  
  # Manual trigger for testing
  workflow_dispatch:
    inputs:
      state:
        description: 'Optional: State filter (e.g., PB or PB,PE,RN)'
        required: false
        default: ''
      provider:
        description: 'Optional: Single provider to run (e.g., corridasemaratonas, ticketsports)'
        required: false
        default: ''
      skip_legacy:
        description: 'Skip legacy corridasemaratonas provider'
        required: false
        type: boolean
        default: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hours max

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: scripts/scraper/package-lock.json

      - name: Install dependencies
        working-directory: scripts/scraper
        run: npm ci

      - name: Generate Prisma Client
        working-directory: scripts/scraper
        run: npx prisma generate

      - name: Install Playwright
        working-directory: scripts/scraper
        run: npx playwright install chromium --with-deps

      - name: Build CLI arguments
        id: args
        run: |
          ARGS=""
          if [ -n "${{ github.event.inputs.state }}" ]; then
            ARGS="$ARGS --state=${{ github.event.inputs.state }}"
          fi
          if [ -n "${{ github.event.inputs.provider }}" ]; then
            ARGS="$ARGS --provider=${{ github.event.inputs.provider }}"
          fi
          if [ "${{ github.event.inputs.skip_legacy }}" == "true" ]; then
            ARGS="$ARGS --skip-legacy"
          fi
          echo "cli_args=$ARGS" >> $GITHUB_OUTPUT

      - name: Run Multi-Provider Scraper
        working-directory: scripts/scraper
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DIRECT_URL: ${{ secrets.DIRECT_URL }}
          API_BASE_URL: ${{ secrets.API_BASE_URL }}
          DETAIL_TIMEOUT_MS: '15000'
          REG_TIMEOUT_MS: '20000'
          SCRAPER_EVENT_DELAY_MS: '500'
          SCRAPER_STALENESS_DAYS: '7'
          LOG_LEVEL: 'info'
        run: npx ts-node src/main.ts ${{ steps.args.outputs.cli_args }}

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: scripts/scraper/logs/
          retention-days: 7
